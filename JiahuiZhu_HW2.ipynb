{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zp_nCL_eeFqv"
      },
      "source": [
        "**IMPORTANT:** \n",
        "\n",
        "**This notebook is a template, and you cannot edit it. To write your own PA, download as .ipynb (File -> download as .ipynb) and upload it to your own Google Colab.**\n",
        "\n",
        "You should run this using a GPU! In Colab, go to the \"runtime\" menu, choose \"change runtime type\", and select \"GPU\" from the hardware accelerator dropdown menu. Otherwise, this might take much longer. With the GPU, it should run in about 20 minutes or less.\n",
        "\n",
        "There are **7** places you need to write code in this notebook, for questions **1, 2a, 2b, 3a, 3b, 4a, 4b**, for a total of 60 points.\n",
        "\n",
        "To turn in this homework: download as .ipynb (File -> download as .ipynb). Make the filename YOURNAME_HW2.ipynb and upload to Gradescope. The assignment due on November 11th at 11:59PM.\n",
        "\n",
        "Be aware: this assignment may require up to **3 hours** of computation for testing different parameters even when\n",
        "everything is implemented correctly.\n",
        "\n",
        "Please read the comments and explanation code carefully. In particular, you may want to study the provided implementation of SGD to get an idea of how the pytorch optimizer class works.\n",
        "\n",
        "Cell outputs are provided. If you implement your code correctly, you should expect your outputs to be similar to the provided outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tohFb7lQUdI7",
        "outputId": "3a3cf540-1d0c-4041-bb6a-f81b78f62fec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'PA2'...\n",
            "remote: Enumerating objects: 24, done.\u001b[K\n",
            "remote: Counting objects: 100% (24/24), done.\u001b[K\n",
            "remote: Compressing objects: 100% (19/19), done.\u001b[K\n",
            "remote: Total 24 (delta 11), reused 16 (delta 5), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (24/24), done.\n",
            "--2022-11-12 05:04:11--  https://raw.githubusercontent.com/amephraim/nlp/master/texts/J.%20K.%20Rowling%20-%20Harry%20Potter%201%20-%20Sorcerer's%20Stone.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 439742 (429K) [text/plain]\n",
            "Saving to: ‘J. K. Rowling - Harry Potter 1 - Sorcerer's Stone.txt’\n",
            "\n",
            "J. K. Rowling - Har 100%[===================>] 429.44K  --.-KB/s    in 0.006s  \n",
            "\n",
            "2022-11-12 05:04:11 (70.8 MB/s) - ‘J. K. Rowling - Harry Potter 1 - Sorcerer's Stone.txt’ saved [439742/439742]\n",
            "\n",
            "--2022-11-12 05:04:11--  https://raw.githubusercontent.com/amephraim/nlp/master/texts/J.%20K.%20Rowling%20-%20Harry%20Potter%202%20-%20The%20Chamber%20Of%20Secrets.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 492130 (481K) [text/plain]\n",
            "Saving to: ‘J. K. Rowling - Harry Potter 2 - The Chamber Of Secrets.txt’\n",
            "\n",
            "J. K. Rowling - Har 100%[===================>] 480.60K  --.-KB/s    in 0.004s  \n",
            "\n",
            "2022-11-12 05:04:12 (122 MB/s) - ‘J. K. Rowling - Harry Potter 2 - The Chamber Of Secrets.txt’ saved [492130/492130]\n",
            "\n",
            "--2022-11-12 05:04:12--  https://raw.githubusercontent.com/amephraim/nlp/master/texts/J.%20K.%20Rowling%20-%20Harry%20Potter%203%20-%20Prisoner%20of%20Azkaban.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 611601 (597K) [text/plain]\n",
            "Saving to: ‘J. K. Rowling - Harry Potter 3 - Prisoner of Azkaban.txt’\n",
            "\n",
            "J. K. Rowling - Har 100%[===================>] 597.27K  --.-KB/s    in 0.004s  \n",
            "\n",
            "2022-11-12 05:04:12 (155 MB/s) - ‘J. K. Rowling - Harry Potter 3 - Prisoner of Azkaban.txt’ saved [611601/611601]\n",
            "\n",
            "--2022-11-12 05:04:12--  https://raw.githubusercontent.com/amephraim/nlp/master/texts/J.%20K.%20Rowling%20-%20Harry%20Potter%204%20-%20The%20Goblet%20of%20Fire.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1109179 (1.1M) [text/plain]\n",
            "Saving to: ‘J. K. Rowling - Harry Potter 4 - The Goblet of Fire.txt’\n",
            "\n",
            "J. K. Rowling - Har 100%[===================>]   1.06M  --.-KB/s    in 0.01s   \n",
            "\n",
            "2022-11-12 05:04:12 (85.0 MB/s) - ‘J. K. Rowling - Harry Potter 4 - The Goblet of Fire.txt’ saved [1109179/1109179]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Clone attention modeling and training code needed for question 3\n",
        "!git clone https://github.com/optmlclass/PA2.git\n",
        "\n",
        "# get harry potter data (also for question 3)\n",
        "# !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "!wget \"https://raw.githubusercontent.com/amephraim/nlp/master/texts/J.%20K.%20Rowling%20-%20Harry%20Potter%201%20-%20Sorcerer's%20Stone.txt\"\n",
        "!wget \"https://raw.githubusercontent.com/amephraim/nlp/master/texts/J.%20K.%20Rowling%20-%20Harry%20Potter%202%20-%20The%20Chamber%20Of%20Secrets.txt\"\n",
        "!wget \"https://raw.githubusercontent.com/amephraim/nlp/master/texts/J.%20K.%20Rowling%20-%20Harry%20Potter%203%20-%20Prisoner%20of%20Azkaban.txt\"\n",
        "!wget \"https://raw.githubusercontent.com/amephraim/nlp/master/texts/J.%20K.%20Rowling%20-%20Harry%20Potter%204%20-%20The%20Goblet%20of%20Fire.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "eV3jEpEcUqOa"
      },
      "outputs": [],
      "source": [
        "import torch, torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.optim import Optimizer\n",
        "import math\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from matplotlib import pyplot as plt\n",
        "from collections.abc import Iterable\n",
        "\n",
        "# set up logging\n",
        "import logging\n",
        "logging.basicConfig(\n",
        "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
        "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
        "        level=logging.INFO,\n",
        ")\n",
        "\n",
        "# make deterministic\n",
        "from PA2.sequenceutils import set_seed\n",
        "set_seed(42)\n",
        "\n",
        "# imports for attention model\n",
        "# more imports\n",
        "from PA2.attentionmodel import GPT, GPTConfig\n",
        "from PA2.sequenceutils import sample, CharDataset\n",
        "from PA2.attentiontrainer import Trainer, TrainerConfig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WMcZz0xk1ff"
      },
      "source": [
        "# Warmup: Standard SGD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jL5FoPbAlCyx"
      },
      "source": [
        "## SGD Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jprhL760UwwY"
      },
      "outputs": [],
      "source": [
        "# Basic SGD implementation for reference.\n",
        "# for base class, see https://github.com/pytorch/pytorch/blob/master/torch/optim/optimizer.py\n",
        "# for official pytorch SGD implementation, see https://github.com/pytorch/pytorch/blob/master/torch/optim/sgd.py\n",
        "\n",
        "class SGD(Optimizer):\n",
        "  def __init__(self, params, lr=1.0):\n",
        "    super(SGD, self).__init__(params, {'lr': lr})\n",
        "\n",
        "    # The params argument can be a list of pytorch variables, or\n",
        "    # a list of dicts. If it is a list of dicts, each dict should have \n",
        "    # a key 'params' that is a list of pytorch variables,\n",
        "    # and optionally another key 'lr' that specifies the learning rate\n",
        "    # for those variables. If 'lr' is not provided, the default value\n",
        "    # is the single value provided as an argument after params to this\n",
        "    # constructor.\n",
        "    # If params is just a list of pytorch variables, it is the same\n",
        "    # as if params were actually a list containing a single dictionary\n",
        "    # whose 'params' key value is the list of variables.\n",
        "    # See examples in following code blocks for use of params.\n",
        "\n",
        "    # Set up an iteration counter.\n",
        "    # self.state[p] is a python dict for each parameter p\n",
        "    # that can be used to store various state useful in the optimization\n",
        "    # algorithm. In this case, we simply store the iteration count, although\n",
        "    # it is not used in this simple algorithm.\n",
        "    for group in self.param_groups:\n",
        "      for p in group['params']:\n",
        "        state = self.state[p]\n",
        "        state['step'] = 0\n",
        "\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def step(self, closure=None):\n",
        "    loss = None\n",
        "    # 'closure' is None for all optimizers we are going to implement in this assignment.\n",
        "    if closure is not None:\n",
        "      with torch.enable_grad():\n",
        "        loss = closure()\n",
        "\n",
        "    for group in self.param_groups:\n",
        "      lr = group['lr']\n",
        "\n",
        "      # it is common practice to call the model parameters p in code.\n",
        "      # in class we follow more closely analytical conventions, in which the\n",
        "      # parameters are often called w for weights.\n",
        "      for p in group['params']:\n",
        "        if p.grad is None:\n",
        "          continue\n",
        "        \n",
        "        # Update the iteration counter (again, this is not actually used in this algorithm)\n",
        "        state = self.state[p]\n",
        "        step = state['step']\n",
        "        step += 1\n",
        "        \n",
        "        # Perform the SGD update. p.grad holds the gradient of the loss\n",
        "        # with respect to p.\n",
        "        p.add_(p.grad, alpha=-lr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3CTrcvelF1h"
      },
      "source": [
        "## Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "B7GJhIPkXICX",
        "outputId": "ce4d43dd-4dfe-420a-a0b6-04d469595978"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "#Simple linear regression problem\n",
        "dimension = 10\n",
        "num_iter = 10000\n",
        "\n",
        "mean = torch.zeros(dimension)\n",
        "std = torch.ones(dimension)\n",
        "\n",
        "def loss_func(w_hat, b_hat, w_true, b_true):\n",
        "  # simple linear regression problem, although\n",
        "  # slightly non-standard loss. See pytorch docs\n",
        "  # for description of loss function.\n",
        "\n",
        "  # features\n",
        "  x = torch.normal(mean, std)\n",
        "\n",
        "  # true label is a linear function of features plus noise.\n",
        "  noise = np.random.normal(0.0, 0.01)\n",
        "  y_true = torch.dot(x, w_true) + b_true + noise\n",
        "\n",
        "  y_hat = torch.dot(x, w_hat) + b_hat\n",
        "\n",
        "  loss = torch.nn.SmoothL1Loss()\n",
        "  return loss(y_hat, y_true)\n",
        "\n",
        "\n",
        "# Set \"true\" parameter value to be a random normal vector with covariance 10*I\n",
        "w_true = 10*torch.normal(mean, std)\n",
        "\n",
        "# make true bias term quite large so that it is better\n",
        "# to have a high learning rate for the bias. This makes\n",
        "# it advantageous to use the params as a dict in the \n",
        "# following cell.\n",
        "b_true = torch.normal(torch.zeros(1), torch.ones(1))\n",
        "\n",
        "\n",
        "# declare variables that will actually be trained.\n",
        "# \"requires_grad\" tells pytorch that it may have to compute\n",
        "# gradients with respect to these variables so that it initialize the \n",
        "# relevant autograd data structures.\n",
        "w = torch.zeros(dimension, requires_grad=True)\n",
        "b = torch.zeros(1, requires_grad=True)\n",
        "\n",
        "optimizer = SGD([w, b], 0.010)\n",
        "\n",
        "losses = []\n",
        "\n",
        "for t in range(num_iter):\n",
        "\n",
        "  # \"zero_grad\" resets all variable.grad values to 0 and resets all\n",
        "  # intermediate data that might have been saved for a backwards pass.\n",
        "  # This is useful when variables need to be reused for many backward passes.\n",
        "  # Note that your PA1 autograd implementation did not need to have this\n",
        "  # functionality because the SGD you implemented just created  new\n",
        "  # variables every iteration. However, it is more efficient to reuse the\n",
        "  # memory from the original variable.\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  # Compute the loss function - this is the forward pass\n",
        "  loss = loss_func(w, b, w_true, b_true)\n",
        "\n",
        "  # loss.backward has essentially the same functionality as the .backward\n",
        "  # function you implemented in PA1.\n",
        "  # In detail, loss.backward computes param.grad for each parameter,\n",
        "  # and optimizer.step updates param.data for each parameter.\n",
        "  loss.backward()\n",
        "\n",
        "  optimizer.step()\n",
        "\n",
        "  losses.append(loss.item())\n",
        "\n",
        "plt.plot(losses)\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('iteration')\n",
        "plt.xscale('log')\n",
        "plt.yscale('log')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "Fo8zvSzpbxgJ",
        "outputId": "49447896-6b01-4039-fa89-b11bcf535988"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "w = torch.zeros(dimension, requires_grad=True)\n",
        "b = torch.zeros(1, requires_grad=True)\n",
        "param_groups = [{'params': [w], 'lr': 0.1}, {'params': [b], 'lr': 0.01}]\n",
        "optimizer = SGD(param_groups, 0.10)\n",
        "\n",
        "losses = []\n",
        "for t in range(num_iter):\n",
        "  optimizer.zero_grad()\n",
        "  loss = loss_func(w, b, w_true, b_true)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  losses.append(loss.item())\n",
        "\n",
        "plt.plot(losses)\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('iteration')\n",
        "plt.xscale('log')\n",
        "plt.yscale('log')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgaaUrMPXCt5"
      },
      "source": [
        "Remarks\n",
        "\n",
        "1. What are these \"`p`\" values representing?\n",
        "\n",
        "In class, we frequently consider a single parameter $w$ to describe the model we are training, so that $w$ would represent all the weights in a neural network for example. However, in practice usually models are specified in a more modular manner. For example, we could have a linear model with a bias as:\n",
        "$$\n",
        "\\hat y = \\langle A, x\\rangle + B\n",
        "$$\n",
        "where $A\\in \\mathbb{R}^d$ is a weight vector and $B\\in\\mathbb{R}$ is a bias. For theoretical analyses, we might consider this model as parametrized by a single vector $w\\in \\mathbb{R}^{d+1}$, but when programming it is far simpler to have separate variables for $A$ and $B$. Just as in your previous programming assignment, the `A.grad` and `B.grad` variables will then be populated with  the partial derivatives of the overall loss with respect to $A$ and $B$. Thus, to implement a gradient based optimizer, we need to loop over all variables that were involved in specifying the model. `p` is the variable used to step through this loop, so in the linear model with bias example, `p` would take on both `A` and `B` as values.\n",
        "\n",
        "2. What does `@torch.no_grad` do? \n",
        "\n",
        "This tells the pytorch not to record gradient operations for anything that happens inside the function, so it will not be able to compute derivatives of these operations. However, this is important to enable because some operations (like performing an update) inherently should not be differentiated, and pytorch will thrown an error if you do not explicitly tell it that you do not want to differentiate through this operation.\n",
        "\n",
        "3. Why do we need `torch.enable_grad` when using the closure?\n",
        "\n",
        "Computing the closure (closure is essentially a fancy name for \"function\") is an alternative to providing the gradient to the optimization step. Instead, the loss $\\ell(w_t, z_t)$ is computed by the closure. However, the closure might need to compute some derivatives, so we turn on `torch.enable_grad`.\n",
        "\n",
        "4. Why do we use `p.add_` as opposed to `p = p - lr*grad`?\n",
        "\n",
        "All pytorch variables (and all python variables except for simple things like integers and chars for that matter) are essentially *pointers*. That is, `p` is a name for an area of computer memory that contains the data for some vector. This makes it fast to pass `p` around to various functions, but if we do `p = z`, this actually means that now `p` refers to the same memory location that `z` is referring to. This is a problem, because the original data has not been changed. `p.add_` instead will change the original data. If you are not familiar with this concept in programming, you may want to do some reading on pass-by-reference vs pass-by-value and pointers.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dF5yZby-iG7b"
      },
      "source": [
        "# PART 1: Per-Variable AdaGrad\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l315KMmyiFGA"
      },
      "source": [
        "## QUESTION 1 (10pt)\n",
        "\n",
        "1. Fill out the class below to implement a version of SGD that sets the \"adaptive\" learning rate on a per-pytorch variable basis:\n",
        "$$\n",
        "\\eta_t[p] = \\frac{lr}{\\sqrt{G_t[p]}}\n",
        "$$\n",
        "$lr$ (for \"learning rate\") is provided as input to the algorithm, $t$ is the current iteration count, and\n",
        "$$\n",
        "G_t[p] = \\sum_{i=1}^t \\|\\nabla \\ell(w_i, z_i)[p]\\|^2\n",
        "$$\n",
        "where here $p$ indicates a pytorch variable object (which could be a high-dimensional vector), and $\\nabla \\ell(w_t, z_t)[p]$ indicates the partial derivative with respect to $p$ (i.e. the coordinates of the gradient corresponding to $p$).\n",
        "If you are familiar with the AdaGrad update, note that this is NOT the same because AdaGrad uses a *per-coordinate* learning rate, while we use a *per-variable* learning rate. For very large models in which memory is at a premium, this kind of change can reduce the memory required by the optimizer while still preserving some of the adaptive properties of AdaGrad.\n",
        "\n",
        "Read the `__init__` method carefully as it has changed slightly from the original `SGD` class above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1BZwfQwgf-i7"
      },
      "outputs": [],
      "source": [
        "class SGD_adaptive(Optimizer):\n",
        "  def __init__(self, params, lr=1.0):\n",
        "    super(SGD_adaptive, self).__init__(params, {'lr': lr})\n",
        "\n",
        "    for group in self.param_groups:\n",
        "      for p in group['params']:\n",
        "        state = self.state[p]\n",
        "        state['step'] = 0\n",
        "\n",
        "        # make a dictionary entry for Gt. It is just a 1-D vector (a scalar).\n",
        "        # device=p.device tells pytorch to allocate the memory for Gt on the \n",
        "        # same device (e.g. CPU or GPU) as the data for the variable p.\n",
        "        state['Gt'] = torch.zeros(1, device=p.device)\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def step(self, closure=None):\n",
        "    loss = None\n",
        "    if closure is not None:\n",
        "      with torch.enable_grad():\n",
        "        loss = closure()\n",
        "\n",
        "    for group in self.param_groups:\n",
        "      lr = group['lr']\n",
        "      for p in group['params']:\n",
        "        if p.grad is None:\n",
        "          continue\n",
        "        \n",
        "        ####### YOUR CODE HERE ########\n",
        "        self.state[p]['Gt'] += torch.linalg.norm(p.grad)\n",
        "        self.state[p]['step'] += 1\n",
        "        e_lr = lr/torch.sqrt(self.state[p]['Gt'])\n",
        "        p.add_(p.grad, alpha=-e_lr.item())\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "AVFwzCFUcewz",
        "outputId": "875a4246-5297-43a8-e6b2-fe6f8a7672fc"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "w = torch.zeros(dimension, requires_grad=True)\n",
        "b = torch.zeros(1, requires_grad=True)\n",
        "optimizer = SGD_adaptive([w, b], 1.0)\n",
        "losses = []\n",
        "for t in range(num_iter):\n",
        "  optimizer.zero_grad()\n",
        "  loss = loss_func(w, b, w_true, b_true)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  losses.append(loss.item())\n",
        "\n",
        "plt.plot(losses)\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('iteration')\n",
        "plt.xscale('log')\n",
        "plt.yscale('log')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owhhpRSrSCJr"
      },
      "source": [
        "See https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#sphx-glr-beginner-blitz-cifar10-tutorial-py for some similar example of how to set up the process below, from which much of this was copied."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JM_sfJdXgkUq",
        "outputId": "5805ea79-9ba6-4c94-9fc4-064604623337"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "## These commands create two pytorch \"DataLoader\" objects, one for training\n",
        "# data and one for testing data.\n",
        "# A DataLoader object is essentially a list of training/testing examples with\n",
        "# some extra code attached by pytorch that can send the data to GPUs.\n",
        "\n",
        "# The following three commands produce a torchvision dataset object for both\n",
        "# training and testing data on the CIFAR10 dataset, which is a dataset of images.\n",
        "# these objects are like lists of vectors (the \"transform\" specifies a function\n",
        "# that converts images to vectors).\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "CIFAR_train = torchvision.datasets.CIFAR10(root='CIFAR10/', train=True, download=True, transform=transform)\n",
        "CIFAR_test = torchvision.datasets.CIFAR10(root='CIFAR10/', train=False, transform=transform)\n",
        "\n",
        "\n",
        "# The next two lines produce the actual DataLoaders. The batch_size argument sets\n",
        "# the batch size, so that the DataLoader will appear like a list [a,b,c...] where\n",
        "# each entry of the list is a batch of batch_size examples. \n",
        "# Setting shuffle=True makes it so that when you start iterating over the examples\n",
        "# in the DataLoader, it will first shuffle the order of the list.\n",
        "trainloader = torch.utils.data.DataLoader(CIFAR_train, batch_size=16,\n",
        "                                          shuffle=True)\n",
        "testloader = torch.utils.data.DataLoader(CIFAR_test, batch_size=256,\n",
        "                                          shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IcoCaPmQhBra",
        "outputId": "c58abedd-4f8e-4cc9-c3bf-8a774b4830e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
        "print(device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qNCQEnkhuQk"
      },
      "source": [
        "## QUESTION 2\n",
        "\n",
        "Function `train_renset` implements the training loop. It loops over the entire dataset multiple time, and each pass is called an \"epoch\". At the beginning of each pass, the entire training data is shuffled and partitioned into mini-batches, and we will loop over every batch $i=1,\\ldots,\\lceil \\frac{N}{b} \\rceil$, where $N$ denotes the size of training data, and $b$ denote the batch size. \n",
        "\n",
        "In this problem, you will need to implement part of `train_resnet` inside the inner loop over mini-batches. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wHGSAS44yhE"
      },
      "source": [
        "### 2a (5pt)\n",
        "\n",
        "In each iteration $i$, let $z_i=(x_i, y_i)$ be $i$-th data batch, where $x_i=(x_i^1,\\ldots,x_i^b)$ is the list of features and $y_i=(y_i^1,\\ldots,y_i^b)$ is the list of labels. You need to compute the cross entropy loss $\\ell(w_i,z_i)$, plus an L2 regularization (otherwise known as \"weight decay\") term to generate the full loss function: $$\\bar\\ell(x_i, z_i) =\\ell(w_i, z_i)+ \\frac{\\lambda}{2}\\|w_i\\|^2$$. In the code, $\\lambda$ is stored in the variable `wd` (partly because `lambda` is a special keyword in python).\n",
        "\n",
        "See [here](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html?highlight=loss#torch.nn.CrossEntropyLoss) for `torch.nn.CrossEntropyLoss`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GiCY7NGp41lq"
      },
      "source": [
        "### 2b (5pt)\n",
        "\n",
        "In this part, you need to call the optimizer and make a step w.r.t. the regularized loss $\\bar\\ell(w_i,z_i)$. *Hint: see the training loop for SGD as a reference*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UQsA0rWgQrUn"
      },
      "outputs": [],
      "source": [
        "## Please read the function signatures and doc string for \"train_resnet\" function.\n",
        "# You do not need to otherwise understand how it works to do this homework. ##\n",
        "\n",
        "def adjust_learning_rate(optimizer, lr):\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "def train_resnet(optimizer, train_loader, device, lrs, wd=0.01, model=None):\n",
        "  '''\n",
        "  Trains a resnet model on CIFAR10 data.\n",
        "\n",
        "  args:\n",
        "    optimizer: optimizer class that will be instantiated inside this function.\n",
        "    train_loader: pytorch DataLoader object that provides the training examples.\n",
        "    device: pytorch device to use (e.g. a GPU).\n",
        "    lrs: list of learning rates. The length of the list is the number of epochs\n",
        "      to train for. At the start of the ith epoch, the 'lr' parameter of the\n",
        "      optimizer will be set to lrs[i].\n",
        "    wd: l2 regularization constant.\n",
        "    model: can provide a preset pytorch model to train from a checkpoint. If \n",
        "      None, will instantiate a fresh resnet18 model.\n",
        "\n",
        "  returns:\n",
        "    the trained pytorch model object.\n",
        "  '''\n",
        "\n",
        "  if model is None:\n",
        "    model = torchvision.models.resnet18(pretrained=False)\n",
        "  model.to(device)\n",
        "\n",
        "\n",
        "  # define \\ell as the cross-entropy function.\n",
        "  # this function as implemented in pytorch actually combines both the softmax\n",
        "  # and the cross entropy function into one. As a result, it is\n",
        "  # is non-negative, smooth, Lipschitz, and convex in its argument, which\n",
        "  # are the predicted scores for various classes output by some model.\n",
        "  # Note that this does NOT necessarily imply that it has these properties\n",
        "  # with respect to the parameters of the model.\n",
        "  cross_entropy_loss = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "  optimizer = SGD_adaptive(model.parameters(), lrs[0])\n",
        "\n",
        "  num_epochs = len(lrs)\n",
        "\n",
        "  average_loss = 0.0\n",
        "\n",
        "  for epoch, lr in enumerate(lrs):\n",
        "    # iterate over training set. One full pass over the training set is called\n",
        "    # an \"epoch\". The number of epochs equals to the length of list 'lrs'.\n",
        "\n",
        "    # manually set learning rate at the beginning of each epoch.\n",
        "    adjust_learning_rate(optimizer, lr)\n",
        "\n",
        "    # external progress bar library\n",
        "    pbar = tqdm(enumerate(train_loader), total=len(train_loader))\n",
        "\n",
        "    for it, data in pbar:\n",
        "      # 'it' is index of example, 'data' is the example (e.g. z_i).\n",
        "      # because shuffle=True when defining the trainloader,\n",
        "      # the training set is shuffled after every complete pass.\n",
        "\n",
        "      # unpack the example data: inputs is a batch of images,\n",
        "      # labels is a batch of labels.\n",
        "      inputs, labels = data\n",
        "      inputs = inputs.to(device)\n",
        "      labels = labels.to(device)\n",
        "\n",
        "      # pytorch will keep details from old gradients around in case you are going\n",
        "      # to differentiate something else. We therefore need to delete this old\n",
        "      # data before computing the loss so that we only have the gradient\n",
        "      # for this iteration.\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # compute labels from model. This will be a tensor of shape [B, C] where B is the \n",
        "      # batch size and C is the number of classes. Each row will contain scores for \n",
        "      # each class, where higher score indicates that the model believe the corresponding\n",
        "      # class is correct.\n",
        "      predicted_labels = model.forward(inputs)\n",
        "\n",
        "\n",
        "      # get the list of parameters in the neural network. This list will\n",
        "      # contain all the weight and bias tensors.\n",
        "      parameters = list(model.parameters())\n",
        "      \n",
        "      ### YOUR CODE HERE (2a) ###\n",
        "      # In this part, you need to implement the regularized loss \\ell(w_i,z_i) + wd/2 * ||w_i||^2.\n",
        "      # You should name your final result as 'regularized_loss'.\n",
        "      # You may wish to use the `cross_entropy_loss` layer defined above. See the\n",
        "      # docs for the CrossEntropyLoss pytorch function to see how to apply it.\n",
        "\n",
        "      pl = 0\n",
        "      for par in parameters:\n",
        "          pl =pl +  wd/2*torch.linalg.norm(par.data) ** 2\n",
        "      regularized_loss = cross_entropy_loss(predicted_labels, labels) + pl\n",
        "\n",
        "\n",
        "      ### YOUR CODE HERE (2b) ###\n",
        "      # In this part, you need to call the optimizer and make a step\n",
        "      # w.r.t. the regularized_loss.\n",
        "      # loss = regularized_loss\n",
        "      regularized_loss.backward()\n",
        "      optimizer.step()\n",
        "      #raise NotImplementedError\n",
        "\n",
        "      # update average loss\n",
        "      average_loss += (loss.item() - average_loss)/(it+1)\n",
        "\n",
        "      pbar.set_description(f\"epoch {epoch + 1} iter {it + 1}: train loss {average_loss:.5f}.\")\n",
        "\n",
        "    average_loss = 0.0\n",
        "\n",
        "  print('\\nFinished Training')\n",
        "\n",
        "  return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ZEClnnmfy7f",
        "outputId": "cb681203-4885-47d0-b2fd-a5a9e0b44499"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 1 iter 3125: train loss 0.00000.: 100%|██████████| 3125/3125 [01:22<00:00, 37.77it/s]\n",
            "epoch 2 iter 3125: train loss 0.00000.: 100%|██████████| 3125/3125 [01:22<00:00, 37.66it/s]\n",
            "epoch 3 iter 3125: train loss 0.00000.: 100%|██████████| 3125/3125 [01:23<00:00, 37.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Finished Training\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "## Train the model for 3 epochs using a learning rate of 0.1 in each epoch.\n",
        "## If this fails, you probably have a significant bug in your SGD code.\n",
        "## Each epoch should take < 5 minutes, for a total of at most 15 minutes.\n",
        "trained_model = train_resnet(SGD_adaptive, trainloader, device, lrs=[0.1, 0.1, 0.1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rbHMj8JOm52O"
      },
      "outputs": [],
      "source": [
        "# These functions take input a trained model and return either the train or\n",
        "# test accuracy.\n",
        "def get_train_accuracy(trained_model):\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  with torch.no_grad():\n",
        "    trainloader = torch.utils.data.DataLoader(CIFAR_train, batch_size=256, num_workers=2)\n",
        "    for data in trainloader:\n",
        "      images, labels = data\n",
        "      images = images.to(device)\n",
        "      labels = labels.to(device)\n",
        "      outputs = trained_model(images)\n",
        "      _, predicted = torch.max(outputs.data, 1)\n",
        "      total += labels.size(0)\n",
        "      correct += (predicted == labels).sum().item()\n",
        "  return 100 * correct / total\n",
        "\n",
        "def get_test_accuracy(trained_model):\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  with torch.no_grad():\n",
        "    testloader = torch.utils.data.DataLoader(CIFAR_test, batch_size=256, num_workers=2)\n",
        "    for data in testloader:\n",
        "      images, labels = data\n",
        "      images = images.to(device)\n",
        "      labels = labels.to(device)\n",
        "      outputs = trained_model(images)\n",
        "      _, predicted = torch.max(outputs.data, 1)\n",
        "      total += labels.size(0)\n",
        "      correct += (predicted == labels).sum().item()\n",
        "  return 100 * correct / total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lFBWPlufSliV",
        "outputId": "a49d5570-efd0-4fea-923b-8c2e9693b8d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test accuracy percentage:  54.65\n"
          ]
        }
      ],
      "source": [
        "## Print the test accuracy. The test accuracy should be ~50% because the learning\n",
        "## rate of 0.1 used earlier is not a good learning rate.\n",
        "test_accuracy = get_test_accuracy(trained_model)\n",
        "print(\"test accuracy percentage: \", test_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ccDYE29-WJ7"
      },
      "source": [
        "## QUESTION 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ft9XCePWpHqE"
      },
      "source": [
        "### 3a (10pt)\n",
        "\n",
        "In the previous section, we choose the learning rates to be 0.1 in all three epochs, which leads to ~50% test accuracy after 3 epochs of training. In this part, tune the learning rate schedules to get the best test performance at batch size 16. You should get close to or better than 70%.\n",
        "\n",
        "Note: \n",
        "With batch size 16, each epoch should take < 5 minutes (for a total of 15 minutes).\n",
        "\n",
        "Hint:\n",
        "- A common practice for tuning learning rates is grid search. You can make a list of candidates (typically in log scale, e.g., `[1,0.1,0.01,0.001]`) and then choose the best learning rate among them.\n",
        "- For this problem, you can assume that the learning rates for later epochs is always less than or equal to the learning rates for earlier epochs, i.e., $\\eta_{t+1} \\le \\eta_t$. You can also assume that $\\eta_{t+1} \\geq \\eta_t/10$. These are also common choices in practice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hjSH4rPceb5J"
      },
      "outputs": [],
      "source": [
        "# This function returns a DataLoader object for the training set that uses\n",
        "# the provided argument as a batch size.\n",
        "def get_trainloader(batch_size):\n",
        "  trainloader = torch.utils.data.DataLoader(\n",
        "      CIFAR_train, batch_size=batch_size, shuffle=True)\n",
        "  return trainloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1gFCeiVB-tD9"
      },
      "outputs": [],
      "source": [
        "# Optional utils functions for saving and loading models\n",
        "def save_model(model, model_name):\n",
        "    torch.save(model.state_dict(), f'{model_name}.pth')\n",
        "\n",
        "def load_model(model_name):\n",
        "    state_dict = torch.load(f'{model_name}.pth')\n",
        "    model = torchvision.models.resnet18(pretrained=False)\n",
        "    model.load_state_dict(state_dict)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zcfsvKefdH1U"
      },
      "outputs": [],
      "source": [
        "### TUNE LEARNING RATE SCHEDULE HERE ###\n",
        "# all you need to do is fill out lrs with appropriate values,\n",
        "# the difficulty is finding those values.\n",
        "lrs = [0.1, 0.01, 0.001]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JbwwhpNF6gEk",
        "outputId": "5c32a25a-ff9c-4b7e-ad39-7c7447210b3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 1 iter 3125: train loss 0.00000.: 100%|██████████| 3125/3125 [01:26<00:00, 36.20it/s]\n",
            "epoch 2 iter 3125: train loss 0.00000.: 100%|██████████| 3125/3125 [01:25<00:00, 36.37it/s]\n",
            "epoch 3 iter 3125: train loss 0.00000.: 100%|██████████| 3125/3125 [01:25<00:00, 36.66it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Finished Training\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[batch size 16] test accuracy: 66\n"
          ]
        }
      ],
      "source": [
        "batch_size_list = [16, 64, 256, 1024]\n",
        "trained_models = {16: None, 64: None, 256: None, 1024: None}\n",
        "\n",
        "trainloader = get_trainloader(batch_size=16)\n",
        "trained_models[16] = train_resnet(\n",
        "    SGD_adaptive, trainloader, device, lrs=lrs)\n",
        "\n",
        "# Print the test accuracy.\n",
        "# Your test accuracy should be close to or better than 70%.\n",
        "test_accuracy = get_test_accuracy(trained_models[16])\n",
        "print(\"[batch size %d] test accuracy: %d\" % (16, test_accuracy))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1M6vtfBEq_xG",
        "outputId": "a6403d0a-35f7-4bd1-ce32-5a2deee84d43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[batch size 16] test accuracy: 66\n"
          ]
        }
      ],
      "source": [
        "# Print the test accuracy.\n",
        "# Your test accuracy should be close to or better than 70%.\n",
        "test_accuracy = get_test_accuracy(trained_models[16])\n",
        "print(\"[batch size %d] test accuracy: %d\" % (16, test_accuracy))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axJ5C7xjrUhb"
      },
      "source": [
        "### 3b (10pt)\n",
        "\n",
        "For each batch size in `[16, 64, 256, 1024]`, we populate the dictionary `trained_models` with a model trained using that batch size for three epochs using `train_resnet`, the `SGD_adaptive` algorithm, and the learning rates you tuned in 2a.\n",
        "\n",
        "When you run the code, you should observe that for larger batch size, (1) the performance is worse and (2) the running time is shorter. Briefly explain the two observations.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "1. (Performance)\n",
        "When batch size increase, accuracy will decrease. When batch size increase, opmizer will update fewer times in each epoch, the model will not converge to the ideal point under same epoch number and learning rate.\n",
        "\n",
        "2. (Running time)\n",
        "When batch size increase, running time will decrease because GPU is good to handle muti same processes in the same time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aMG7M0pyYY8M",
        "outputId": "7aa7185e-4274-4a7c-96d5-a36607dd13a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 1 iter 782: train loss 0.00000.: 100%|██████████| 782/782 [00:34<00:00, 22.71it/s]\n",
            "epoch 2 iter 782: train loss 0.00000.: 100%|██████████| 782/782 [00:34<00:00, 22.40it/s]\n",
            "epoch 3 iter 782: train loss 0.00000.: 100%|██████████| 782/782 [00:33<00:00, 23.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Finished Training\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 1 iter 196: train loss 0.00000.: 100%|██████████| 196/196 [00:19<00:00, 10.01it/s]\n",
            "epoch 2 iter 196: train loss 0.00000.: 100%|██████████| 196/196 [00:20<00:00,  9.64it/s]\n",
            "epoch 3 iter 196: train loss 0.00000.: 100%|██████████| 196/196 [00:19<00:00, 10.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Finished Training\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 1 iter 49: train loss 0.00000.: 100%|██████████| 49/49 [00:16<00:00,  2.95it/s]\n",
            "epoch 2 iter 49: train loss 0.00000.: 100%|██████████| 49/49 [00:16<00:00,  3.06it/s]\n",
            "epoch 3 iter 49: train loss 0.00000.: 100%|██████████| 49/49 [00:16<00:00,  2.94it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Finished Training\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "## With batch size 16, each epoch should take < 5 minutes (for a total of 15 minutes).\n",
        "## With batch size 64, < 2 minutes.\n",
        "## With batch size 256 and 1024, < 1 minute.\n",
        "for batch_size in batch_size_list[1:]:\n",
        "    trainloader = get_trainloader(batch_size)\n",
        "    trained_models[batch_size] = train_resnet(\n",
        "        SGD_adaptive, trainloader, device, lrs=lrs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sRcHDkyi-lUT",
        "outputId": "75b6e3b6-09de-4fc6-85e1-5c9ba441efea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[batch size 16] train accuracy: 70 , test accuracy: 66\n",
            "[batch size 64] train accuracy: 71 , test accuracy: 64\n",
            "[batch size 256] train accuracy: 70 , test accuracy: 60\n",
            "[batch size 1024] train accuracy: 59 , test accuracy: 53\n"
          ]
        }
      ],
      "source": [
        "for batch_size in batch_size_list:\n",
        "  test_accuracy = get_test_accuracy(trained_models[batch_size])\n",
        "  train_accuracy = get_train_accuracy(trained_models[batch_size])\n",
        "  print(\"[batch size %d] train accuracy: %d , test accuracy: %d\" % (batch_size, train_accuracy, test_accuracy))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7-22PKTRal_"
      },
      "source": [
        "# PART 2: AdamW and Norm-scaling\n",
        "\n",
        "The next part of this homework assignment investigates implementing some variants of Adam. We will be testing your optimizers on a simplified implementation of [GPT](https://github.com/openai/gpt-3) based on the [minGPT](https://github.com/karpathy/minGPT) repository by Andrej Karpathy. This is a model that takes as input a sequence of 128 characters from a text file and attempts to predict the next character. This can be used to generate novel text by starting with a seed text string, and then repeatedly using the model to generate another character.\n",
        "\n",
        "\n",
        "# Tips\n",
        "* You will need a GPU for this assignment. When using google colab, go to runtime->change runtime type and make sure that the type is set to GPU.\n",
        "\n",
        "* You may decrease the number of training epochs while debugging, but please set it back to 20 and run again before submission.\n",
        "\n",
        "* Study the provided AdaGrad implementation closely, it introduces a few pytorch functions that may be useful. You should check the documentation for these functions to see what they do.\n",
        "\n",
        "* You may occasionally need to restart the runtime (runtime->restart runtime). Sometimes the GPUs don't release memory properly, and sometimes the progress bars get a little messed up."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkPmrmHQPdtO"
      },
      "source": [
        "## QUESTION 4a (10pt)\n",
        "\n",
        "Implement the [AdamW](https://openreview.net/pdf?id=Bkg6RiCqY7) update *without* using the debiasing terms. AdamW performs the following (**per-coordinate**) update:\n",
        "\n",
        "$$\n",
        "w_{t+1} = w_t - \\eta_t\\left(\\frac{\\hat m_t}{\\sqrt{\\hat v_t} +\\epsilon} + \\lambda w_t\\right)\n",
        "$$\n",
        "where $\\hat m_t$ and $\\hat v_t$ are generated the same way as in the standard [Adam](https://openreview.net/pdf?id=Bkg6RiCqY7) update, and $\\lambda$ is an extra \"weight decay\" parameter provided to the optimizer and $\\eta_t$ is the learning rate parameter provided to the optimizer by the user.\n",
        "\n",
        "Ordinarily, \"weight decay\" is another word for L2 regularization. That is, the loss is modified to:\n",
        "$$\n",
        "\\mathcal{L}(w) + \\frac{\\lambda}{2}\\|w\\|^2\n",
        "$$\n",
        "This means that we could implement weight decay by changing the gradient to $\\nabla \\mathcal{L}(w) + \\lambda w$. The idea behind AdamW is that the weight-decay term is in some sense \"well-understood\" and should not be included in the $v_t$ and $A_t$ statistics that are being used to understand the more mysterious loss surface $\\mathcal{L}(w)$. See the linked paper for more details and full pseudocode.\n",
        "\n",
        "In your implementation, you should use the raw $m_t$ and $v_t$ values without applying the debiasing terms discussed in the papers and class. That is, you should implement the update:\n",
        "\n",
        "$$\n",
        "w_{t+1} = w_t - \\eta_t\\left(\\frac{m_t}{\\sqrt{v_t} +\\epsilon} + \\lambda w_t\\right)\n",
        "$$\n",
        "where $m_t$ and $v_t$ are as defined in the [linked paper](https://openreview.net/pdf?id=Bkg6RiCqY7).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wP38enSOjF0Q"
      },
      "source": [
        "## QUESTION 4b (10pt)\n",
        "\n",
        "Upgrade your debias-free AdamW implementation to use the `use_norm_scaling` argument of the `__init__` method. When this argument is `True`, you should scale the learning rate by the norm of the weights *for the given pytorch variable*. That is, for each variable $p$ you will replace the learning rate $\\eta_t$ at time $t$ with $\\|p\\|\\eta_t$ in the update:\n",
        "$$\n",
        "w_{t+1}[i] = w_t[i] - \\|w_t\\|_2\\eta_t\\left(\\frac{m_t[i]}{\\sqrt{v_t[i]} +\\epsilon} + \\lambda w_t[i]\\right)\n",
        "$$\n",
        "When the `use_norm_scaling` argument is false, simply perform the update from question 1a.\n",
        "\n",
        "This learning rate heuristic is inspired by a similar proposal for use with normalized updates in the [LARS](https://arxiv.org/abs/1708.03888) optimizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3TAFPkUfSS70"
      },
      "outputs": [],
      "source": [
        "\n",
        "class AdamW_bias(Optimizer):\n",
        "  def __init__(self, params, lr=1.0, betas=(0.9,0.999), use_norm_scaling=False):\n",
        "    super(AdamW_bias, self).__init__(params, {'lr': lr, 'beta1': betas[0], 'beta2': betas[1], 'weight_decay': 0.0})\n",
        "\n",
        "    self.use_norm_scaling = use_norm_scaling\n",
        "\n",
        "    for group in self.param_groups:\n",
        "      for p in group['params']:\n",
        "        ## YOUR CODE HERE ##\n",
        "        state = self.state[p]\n",
        "        state['step'] = 0\n",
        "        d = len(p)\n",
        "        state['m'] = torch.zeros(p.shape, device = p.device)\n",
        "        state['v'] = torch.zeros(p.shape, device = p.device) \n",
        "        # raise NotImplementedError\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def step(self, closure=None):\n",
        "    # in this class, and also usually in practice, closure will always be None.\n",
        "    loss = None\n",
        "    epsilon = 1e-8\n",
        "    if closure is not None:\n",
        "      with torch.enable_grad():\n",
        "        loss = closure()\n",
        "      \n",
        "    ## YOUR CODE HERE ##\n",
        "    for group in self.param_groups:\n",
        "      lr = group['lr']\n",
        "      beta1 = group['beta1']\n",
        "      beta2 = group['beta2']\n",
        "      weight_decay = group['weight_decay']\n",
        "      for p in group['params']:\n",
        "        if p.grad is None:\n",
        "          continue\n",
        "        state = self.state[p]\n",
        "        g = p.grad\n",
        "        state['step'] += 1\n",
        "        state['m'] = beta1 * state['m'] + (1 - beta1) * g\n",
        "        state['v'] = beta2 * state['v'] + (1 - beta2) * g**2\n",
        "        if self.use_norm_scaling:\n",
        "          p.add_(-lr*(state['m']/(torch.sqrt(state['v'])+epsilon) + weight_decay*p.data), alpha = torch.norm(p))\n",
        "        else:\n",
        "          p.add_(-lr*(state['m']/(torch.sqrt(state['v'])+epsilon) + weight_decay*p.data))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "grKNXrsabE2z",
        "outputId": "db776383-163a-4623-ca92-2b0c4684fec0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "round 1:\n",
            "- your outputs:\n",
            "parameter: tensor([1.1414, 1.1414]), gradient: tensor([-3.4952, -0.6046])\n",
            "states: {'step': 1, 'm': tensor([-0.3495, -0.0605]), 'v': tensor([0.1222, 0.0037])}\n",
            "- reference outputs:\n",
            "parameter: tensor([1.1414, 1.1414])\n",
            "states: {'step': 1, 'mt': tensor([-0.3495, -0.0605]), 'vt': tensor([0.1222, 0.0037])}\n",
            "\n",
            "round 2:\n",
            "- your outputs:\n",
            "parameter: tensor([1.2077, 1.0571]), gradient: tensor([1.5784, 1.2855])\n",
            "states: {'step': 2, 'm': tensor([-0.1567,  0.0741]), 'v': tensor([0.1459, 0.0201])}\n",
            "- reference outputs:\n",
            "parameter: tensor([1.2077, 1.0571])\n",
            "states: {'step': 2, 'mt': tensor([-0.1567,  0.0741]), 'vt': tensor([0.1459, 0.0201])}\n",
            "\n",
            "round 3:\n",
            "- your outputs:\n",
            "parameter: tensor([1.3430, 0.8797]), gradient: tensor([-2.3634,  2.8063])\n",
            "states: {'step': 3, 'm': tensor([-0.3774,  0.3474]), 'v': tensor([0.2003, 0.0987])}\n",
            "- reference outputs:\n",
            "parameter: tensor([1.3430, 0.8797])\n",
            "states: {'step': 3, 'mt': tensor([-0.3774,  0.3474]), 'vt': tensor([0.2003, 0.0987])}\n"
          ]
        }
      ],
      "source": [
        "# a simple bug testing for AdamW implementation for part A *only*.\n",
        "# your outputs should be the same as the reference function.\n",
        "# the internal state will likely be the same as well, but may be different if your\n",
        "# implementation is very different in some way.\n",
        "\n",
        "d = 2\n",
        "lr = 0.1\n",
        "beta1 = 0.9\n",
        "beta2 = 0.99\n",
        "use_norm_scaling = True\n",
        "epsilon = 1e-8\n",
        "\n",
        "def log_state(optim):\n",
        "    state = optim.state\n",
        "    for group in optim.param_groups:\n",
        "        for p in group['params']:\n",
        "            print(f'parameter: {p.data}, gradient: {p.grad}')\n",
        "            print(f'states: {state[p]}')\n",
        "\n",
        "# reference function\n",
        "def reference(p, m, v, t, g):\n",
        "    m = beta1*m + (1-beta1)*g\n",
        "    v = beta2*v + (1-beta2)*g**2\n",
        "    est = m / (epsilon + torch.sqrt(v))\n",
        "    if use_norm_scaling:\n",
        "        p = p - lr*torch.norm(p) * est\n",
        "    else:\n",
        "        p = p - lr * est\n",
        "    print(f'parameter: {p}')\n",
        "    print(f\"states: {{'step': {t}, 'mt': {m}, 'vt': {v}}}\")\n",
        "    return p, m, v, t+1\n",
        "\n",
        "p = torch.ones((d,))\n",
        "optim = AdamW_bias([p], lr=lr, betas=(beta1, beta2), \n",
        "                   use_norm_scaling=use_norm_scaling)\n",
        "\n",
        "p_ref = torch.ones((d,))\n",
        "m = torch.zeros((d,))\n",
        "v = torch.zeros((d,))\n",
        "t = 1\n",
        "\n",
        "for i in range(3):\n",
        "    print(f\"\\nround {i+1}:\")\n",
        "    print(\"- your outputs:\")\n",
        "    p.grad = 10 * torch.rand((d,)) - 5\n",
        "    optim.step()\n",
        "    log_state(optim)\n",
        "    print(\"- reference outputs:\")\n",
        "    p_ref, m, v, t = reference(p_ref, m, v, t, p.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDRm96mkpPCZ"
      },
      "source": [
        "The following cells tests your optimizer on the min-GPT model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EXc_WnyVSOmI",
        "outputId": "52f947a0-6e26-487a-dff5-132464b9806f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data has 2652652 characters, 91 unique.\n"
          ]
        }
      ],
      "source": [
        "# the \"block size\" is the number of characters the model takes as input.\n",
        "# in this case, it can look at up to 128 characters when predicting the next\n",
        "# character.\n",
        "block_size = 128 # spatial extent of the model for its context\n",
        "\n",
        "# For our training set, we will use the text of the first four Harry Potter books.\n",
        "text = open(\"J. K. Rowling - Harry Potter 1 - Sorcerer's Stone.txt\", 'rb').read()\n",
        "text += open('J. K. Rowling - Harry Potter 2 - The Chamber Of Secrets.txt', 'rb').read()\n",
        "text += open('J. K. Rowling - Harry Potter 3 - Prisoner of Azkaban.txt', 'rb').read()\n",
        "text += open('J. K. Rowling - Harry Potter 4 - The Goblet of Fire.txt', 'rb').read()\n",
        "\n",
        "# text = open('input.txt', 'r').read() # don't worry we won't run out of file handles\n",
        "train_dataset = CharDataset(text, block_size) # one line of poem is roughly 50 characters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EFTg91uRPthh"
      },
      "outputs": [],
      "source": [
        "# generate the configuration for the model. These parameters specify\n",
        "# the neural network architecture we will be using. It is not necessary\n",
        "# to understand this model architecture.\n",
        "mconf = GPTConfig(train_dataset.vocab_size, train_dataset.block_size,\n",
        "                  n_layer=8, n_head=8, n_embd=128)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iLKHtyUsPuEl"
      },
      "outputs": [],
      "source": [
        "# generate training configurations for each of the optimizers. We will be testing\n",
        "# adam (official pytorch implementation)\n",
        "# adamw (official pytorch implementation)\n",
        "# your optimizer both with and without the norm_scaling flag set.\n",
        "def adamw_bias_factory(params, lr, betas):\n",
        "  return AdamW_bias(params, lr, betas)\n",
        "\n",
        "def adamw_bias_norm_scaling_factory(params, lr, betas):\n",
        "  return AdamW_bias(params, lr, betas, use_norm_scaling=True)\n",
        "\n",
        "optimizers = {\n",
        "    'adam': torch.optim.Adam, \n",
        "    'adamw': torch.optim.AdamW, \n",
        "    'adamw_bias': adamw_bias_factory, \n",
        "    'adamw_bias_norm_scaling': adamw_bias_norm_scaling_factory\n",
        "  }\n",
        "\n",
        "training_configs = {}\n",
        "\n",
        "for name, opt in optimizers.items():\n",
        "# construct a training config: this sets the learning rate, batch size, number \n",
        "# of epochs ect for each optimizer. warmup_tokens and final_tokens are parameters\n",
        "# used to setup a warm-up and decay learning rate scheduler.\n",
        "  training_configs[name] = TrainerConfig(max_epochs=5, batch_size=256, learning_rate=6e-4, optimizer=opt,\n",
        "                        lr_decay=True, warmup_tokens=512*20, final_tokens=200*len(train_dataset)*block_size,\n",
        "                        num_workers=2)\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_gSApvJa_zte"
      },
      "outputs": [],
      "source": [
        "import gc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "faU9I2fR_0-K",
        "outputId": "b6fb5ca3-93df-4e02-fb52-84a8f9e1b314"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "298"
            ]
          },
          "metadata": {},
          "execution_count": 115
        }
      ],
      "source": [
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GxFY3AXTPvLf",
        "outputId": "31d50563-40bd-4f41-ce78-1808341fc1a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training new model with optimizer: adam\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 1 iter 80: train loss 2.67471. lr 5.999633e-04: 100%|██████████| 81/81 [00:24<00:00,  3.24it/s]\n",
            "epoch 2 iter 80: train loss 2.61293. lr 5.998525e-04: 100%|██████████| 81/81 [00:25<00:00,  3.16it/s]\n",
            "epoch 3 iter 80: train loss 2.62358. lr 5.996678e-04: 100%|██████████| 81/81 [00:25<00:00,  3.18it/s]\n",
            "epoch 4 iter 80: train loss 2.60039. lr 5.994091e-04: 100%|██████████| 81/81 [00:25<00:00,  3.15it/s]\n",
            "epoch 5 iter 80: train loss 2.61173. lr 5.990766e-04: 100%|██████████| 81/81 [00:26<00:00,  3.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data has 2652652 characters, 91 unique.\n",
            "final epoch train loss: 2.6159590026478696\n",
            "training new model with optimizer: adamw\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 1 iter 80: train loss 2.53138. lr 5.999633e-04: 100%|██████████| 81/81 [00:26<00:00,  3.09it/s]\n",
            "epoch 2 iter 80: train loss 2.40038. lr 5.998525e-04: 100%|██████████| 81/81 [00:26<00:00,  3.09it/s]\n",
            "epoch 3 iter 80: train loss 2.24619. lr 5.996678e-04: 100%|██████████| 81/81 [00:26<00:00,  3.10it/s]\n",
            "epoch 4 iter 80: train loss 2.14337. lr 5.994091e-04: 100%|██████████| 81/81 [00:26<00:00,  3.08it/s]\n",
            "epoch 5 iter 80: train loss 1.93592. lr 5.990766e-04: 100%|██████████| 81/81 [00:26<00:00,  3.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data has 2652652 characters, 91 unique.\n",
            "final epoch train loss: 2.028879724902871\n",
            "training new model with optimizer: adamw_bias\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 1 iter 80: train loss 2.49404. lr 5.999633e-04: 100%|██████████| 81/81 [00:26<00:00,  3.07it/s]\n",
            "epoch 2 iter 80: train loss 2.39042. lr 5.998525e-04: 100%|██████████| 81/81 [00:26<00:00,  3.06it/s]\n",
            "epoch 3 iter 80: train loss 2.28602. lr 5.996678e-04: 100%|██████████| 81/81 [00:26<00:00,  3.07it/s]\n",
            "epoch 4 iter 80: train loss 2.10476. lr 5.994091e-04: 100%|██████████| 81/81 [00:26<00:00,  3.05it/s]\n",
            "epoch 5 iter 80: train loss 1.94891. lr 5.990766e-04: 100%|██████████| 81/81 [00:26<00:00,  3.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data has 2652652 characters, 91 unique.\n",
            "final epoch train loss: 2.01365422319483\n",
            "training new model with optimizer: adamw_bias_norm_scaling\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 1 iter 80: train loss 2.50003. lr 5.999633e-04: 100%|██████████| 81/81 [00:27<00:00,  2.98it/s]\n",
            "epoch 2 iter 80: train loss 2.38791. lr 5.998525e-04: 100%|██████████| 81/81 [00:27<00:00,  2.99it/s]\n",
            "epoch 3 iter 80: train loss 2.26936. lr 5.996678e-04: 100%|██████████| 81/81 [00:27<00:00,  2.99it/s]\n",
            "epoch 4 iter 80: train loss 1.97803. lr 5.994091e-04: 100%|██████████| 81/81 [00:27<00:00,  2.99it/s]\n",
            "epoch 5 iter 80: train loss 1.80661. lr 5.990766e-04: 100%|██████████| 81/81 [00:27<00:00,  2.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data has 2652652 characters, 91 unique.\n",
            "final epoch train loss: 1.8888056660875856\n",
            "best optimizer: adamw_bias_norm_scaling with loss: 1.8888056660875856\n"
          ]
        }
      ],
      "source": [
        "# train a model on each optimizer, keeping track of the best-performing one.\n",
        "# each epoch should take less than 2 minutes to train, for a total of at most\n",
        "# 4 algorithms * 5 epochs / algorithm * 2 minutes / epoch = 40 minutes training time\n",
        "losses = {}\n",
        "min_loss = float('inf')\n",
        "best_model = None\n",
        "best_optimizer = None\n",
        "for name, tconf in training_configs.items():\n",
        "  print(\"training new model with optimizer: {}\".format(name))\n",
        "  model = GPT(mconf)\n",
        "  trainer = Trainer(model, train_dataset, None, tconf)\n",
        "  train_loss = trainer.train()\n",
        "  losses[name] = train_loss\n",
        "  train_dataset = CharDataset(text, block_size)\n",
        "  print(\"final epoch train loss: {}\".format(train_loss))\n",
        "  if train_loss < min_loss:\n",
        "    best_optimizer = name\n",
        "    min_loss = train_loss\n",
        "\n",
        "print(\"best optimizer: {} with loss: {}\".format(best_optimizer, min_loss))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J1pDN13SGxf9",
        "outputId": "554f568b-c8e6-4685-a24e-7f61f0a8c3d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training new model with optimizer: adamw_bias_norm_scaling\n",
            "data has 2652652 characters, 91 unique.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 1 iter 80: train loss 2.47840. lr 5.999633e-04: 100%|██████████| 81/81 [00:27<00:00,  2.99it/s]\n",
            "epoch 2 iter 80: train loss 2.40797. lr 5.998525e-04: 100%|██████████| 81/81 [00:27<00:00,  2.95it/s]\n",
            "epoch 3 iter 80: train loss 2.26146. lr 5.996678e-04: 100%|██████████| 81/81 [00:27<00:00,  2.99it/s]\n",
            "epoch 4 iter 80: train loss 1.99882. lr 5.994091e-04: 100%|██████████| 81/81 [00:27<00:00,  2.99it/s]\n",
            "epoch 5 iter 80: train loss 1.74424. lr 5.990766e-04: 100%|██████████| 81/81 [00:27<00:00,  2.99it/s]\n",
            "epoch 6 iter 80: train loss 1.59826. lr 5.986703e-04: 100%|██████████| 81/81 [00:27<00:00,  2.99it/s]\n",
            "epoch 7 iter 80: train loss 1.53731. lr 5.981902e-04: 100%|██████████| 81/81 [00:26<00:00,  3.00it/s]\n",
            "epoch 8 iter 80: train loss 1.52006. lr 5.976366e-04: 100%|██████████| 81/81 [00:27<00:00,  2.99it/s]\n",
            "epoch 9 iter 80: train loss 1.49131. lr 5.970096e-04: 100%|██████████| 81/81 [00:27<00:00,  2.97it/s]\n",
            "epoch 10 iter 80: train loss 1.51333. lr 5.963092e-04: 100%|██████████| 81/81 [00:26<00:00,  3.01it/s]\n",
            "epoch 11 iter 80: train loss 1.45732. lr 5.955358e-04: 100%|██████████| 81/81 [00:27<00:00,  2.99it/s]\n",
            "epoch 12 iter 80: train loss 1.41190. lr 5.946894e-04: 100%|██████████| 81/81 [00:27<00:00,  3.00it/s]\n",
            "epoch 13 iter 80: train loss 1.43415. lr 5.937703e-04: 100%|██████████| 81/81 [00:27<00:00,  3.00it/s]\n",
            "epoch 14 iter 80: train loss 1.42086. lr 5.927787e-04: 100%|██████████| 81/81 [00:27<00:00,  2.99it/s]\n",
            "epoch 15 iter 80: train loss 1.35967. lr 5.917149e-04: 100%|██████████| 81/81 [00:27<00:00,  3.00it/s]\n",
            "epoch 16 iter 80: train loss 1.40788. lr 5.905791e-04: 100%|██████████| 81/81 [00:27<00:00,  2.97it/s]\n",
            "epoch 17 iter 80: train loss 1.37459. lr 5.893717e-04: 100%|██████████| 81/81 [00:27<00:00,  2.99it/s]\n",
            "epoch 18 iter 80: train loss 1.46355. lr 5.880928e-04: 100%|██████████| 81/81 [00:27<00:00,  2.99it/s]\n",
            "epoch 19 iter 80: train loss 1.36857. lr 5.867428e-04: 100%|██████████| 81/81 [00:26<00:00,  3.00it/s]\n",
            "epoch 20 iter 80: train loss 1.34435. lr 5.853221e-04: 100%|██████████| 81/81 [00:27<00:00,  2.98it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "final epoch train loss: 1.3787673387998416\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "## Now that we have identified a best optimizer, let us train for a longer amount of time (another 40 minutes):\n",
        "long_train_conf = TrainerConfig(max_epochs=20, batch_size=256, learning_rate=6e-4, optimizer=optimizers[best_optimizer],\n",
        "                        lr_decay=True, warmup_tokens=512*20, final_tokens=200*len(train_dataset)*block_size,\n",
        "                        num_workers=2)\n",
        "print(\"training new model with optimizer: {}\".format(name))\n",
        "best_model = GPT(mconf)\n",
        "train_dataset = CharDataset(text, block_size)\n",
        "trainer = Trainer(best_model, train_dataset, None, long_train_conf)\n",
        "train_loss = trainer.train()\n",
        "print(\"final epoch train loss: {}\".format(train_loss))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oE48jgJLPwvT",
        "outputId": "aff9360e-c744-4839-a6b1-d618cc174bec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Harry opened the door, and her followed in the floor. Harry had botten a living to the windows.  \"It was a large would be a seconder.\"\n",
            "\"If you will be something in the top of the school beam.\"\n",
            "\"Harry,\" said Fred, turning the sight as though he was looking and speaking as though to the wall, though they were doing to do, the bed of sign of the sight white face was their hour feets.\n",
            "\"Have to deep to have to been a serious time,\" said Denath Eater Bertha Couldre Cedric.\n",
            "\n",
            "\"Won't be to the sine of the station, to have to do in the table.\"\n",
            "\n",
            "\"I took you a still boys there and to stell you and wanted to them,\" said Ron, turning\n",
            "thisk abrupt and left into the wand, whose was an owls.\n",
            "\n",
            "The window had been told\n",
            "the time of the way.\n",
            "\n",
            "There was thinging to the stops of the taile soft.\n",
            "\t\tA still table with three, with a look,\n",
            "\n",
            "\tThe Dark Ark Seamus Cream and Cristered, who was the castle with a score over the table\n",
            "\t\tHarry had never shouted himself open, the watch the first completed to sit a stop and\n",
            "\n",
            "\"And I was getting to him of the storm as he would be time.\"\n",
            "\n",
            "\"The Ministry of Magic Colin Crouch was a bother with the should of the back of the student tower the wand\n",
            "of the tent,\" he asked her.\n",
            "\n",
            "There was not sure.\n",
            "\n",
            "\"That would be what you some wish to their window of him was going to kill his teach, and the tope to thing hasen hand.\"\n",
            "\n",
            "\"The Ministry's teacher and we soften.\"\n",
            "\n",
            "He was as they were to dread from Harry and his face\n",
            "worked. He spoked the shope as he was a continued worried and looking to the stop of the face were.\n",
            "\n",
            "\"What's the boy and and sort of should the sign of his wind.\"\n",
            "\n",
            "\"Harry!\"\n",
            "\n",
            "\"What was throwing the top of yes. There was a been telling a lot of the wall while. He left\n",
            "a smoult tea of the contents. This was shouldn't stopped. \"It was sorting, you\n",
            "to take in the stands. I magic a long of second stream to stay to brand as hours\n",
            "whispers, after the steps.\n",
            "\n",
            "Tower, the first tere, and was the silvery time out to be his feet, the shadowl here.\n",
            "\n",
            "He took the stand again, took a long around th\n"
          ]
        }
      ],
      "source": [
        "# alright, sample some harry potter at the character level, starting from the seed\n",
        "# \"Harry opened the door, and\"\n",
        "# Note, this model is not advanced enough to learn how to actualy write sentences\n",
        "# that makes sense, but it CAN learn to write complete words and certain names.\n",
        "# With longer training times, bigger models, and more data, it is possible to\n",
        "# achieve truly amazing results!\n",
        "\n",
        "context = [ord(c) for c in \"Harry opened the door, and\"]\n",
        "x = torch.tensor([train_dataset.stoi[s] for s in context], dtype=torch.long)[None,...].to(trainer.device)\n",
        "y = sample(best_model, x, 2000, temperature=0.9, sample=True, top_k=5)[0]\n",
        "completion = ''.join([chr(train_dataset.itos[int(i)]) for i in y])\n",
        "print(completion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sGwWZ1Ppy0SW"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.10.7 64-bit (microsoft store)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "46d477783de48e1d09fb6b0616d0d10e7815d6d95e3a5f6184d7f1bf32f0f1f8"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}